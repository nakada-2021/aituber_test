
version: "3.8"

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LOCAL_LLM=${LOCAL_LLM}
      - LLM_URL=${LLM_URL}
    depends_on:
      - llama_cpp

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules

  llama_cpp:
    build:
      context: .
      dockerfile: Dockerfile.llama
    volumes:
      - /Users/nakatatakayuki/aituber_test/models:/models
    ports:
      - "5001:5000"
    entrypoint: python3 -m llama_cpp.server
    command: --model /models/mistral-7b-instruct-v0.2.Q2_K.gguf --host 0.0.0.0 --port 5000 --n_gpu_layers 0
